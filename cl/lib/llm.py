import instructor
from pydantic import BaseModel


def call_llm(
    system_prompt: str,
    prompt: str,
    user_text: str,
    model: str = "openai/gpt-4o-mini",
    response_model: type[BaseModel] | None = None,
    temperature: float = 0.0,
    max_completion_tokens: int = 1000,
) -> BaseModel | dict | str:
    """Call an LLM via Instructor to get structured or raw output

    You must set any of these environment variables for some providers:
    OPENAI_API_KEY, ANTHROPIC_API_KEY or MISTRAL_API_KEY

    :param system_prompt: Instructions as system prompt for the LLM
    :param prompt: Task-specific prompt content
    :param user_text: Text input
    :param model: Instructor provider/model identifier (e.g., "openai/gpt-4o-mini")
    :param response_model: Optional Pydantic model to validate and return typed output
    :param temperature: This value controls the randomness of the text generated by the model
    :param max_completion_tokens: Maximum tokens for generated completion
    :return: Parsed Pydantic model instance if response_model is set, else raw dict or string
    """
    client = instructor.from_provider(model)
    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {"type": "text", "text": user_text},
            ],
        },
    ]

    response = client.chat.completions.create(
        messages=messages,
        temperature=temperature,
        max_tokens=max_completion_tokens,
        response_model=response_model,
    )
    return response
