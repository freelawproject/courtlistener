import instructor
from pydantic import BaseModel


def call_llm(
    system_prompt: str,
    prompt: str | list[str] | list[dict],
    model: str = "openai/gpt-4o-mini",
    response_model: type[BaseModel] | None = None,
    temperature: float = 0.0,
    max_completion_tokens: int = 1000,
    api_key: str | None = None,
) -> BaseModel | dict | str:
    """Call an LLM via Instructor to get structured or raw output

    You must set any of these environment variables for some providers:
    OPENAI_API_KEY, ANTHROPIC_API_KEY or MISTRAL_API_KEY

    :param system_prompt: Instructions as system prompt for the LLM
    :param prompt: Task-specific prompt content, it may be: a single string, a list of strings, a list of prebuilt
    content parts like {"type": "text", "text": "..."}
    :param model: Instructor provider/model identifier (e.g., "openai/gpt-4o-mini")
    :param response_model: Optional Pydantic model to validate and return typed output
    :param temperature: This value controls the randomness of the text generated by the model
    :param max_completion_tokens: Maximum tokens for generated completion
    :param api_key: Optional explicit API key for the provider
    :return: Parsed Pydantic model instance if response_model is set, else raw dict or string
    """

    # if api_key is provided, inject it, else fallback to env var
    client = instructor.from_provider(model, api_key=api_key)

    def to_content_part(x: str | dict) -> dict:
        if isinstance(x, str):
            return {"type": "text", "text": x}
        # Assume already a valid content part dict, e.g. {"type": "text", "text": "..."}
        return x

    if isinstance(prompt, str):
        user_content = [to_content_part(prompt)]
    else:
        user_content = [to_content_part(p) for p in prompt]

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content},
    ]

    response = client.chat.completions.create(
        messages=messages,
        temperature=temperature,
        max_tokens=max_completion_tokens,
        response_model=response_model,
    )
    return response
