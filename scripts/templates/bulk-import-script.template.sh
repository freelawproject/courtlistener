#!/bin/bash
set -e

# Advanced bulk data import script generated by CourtListener bulk data pipeline
# Implements PLAN.md features: split schema, inconsistency detection, remediation

# ANSI color codes for beautiful output
RED='\033[1;31m'
GREEN='\033[1;32m'
YELLOW='\033[1;33m'
BLUE='\033[1;34m'
CYAN='\033[1;36m'
BOLD='\033[1m'
NC='\033[0m' # No Color

WORKERS=-1
SKIP_REMEDIATION=0
AUTO_REMEDIATE="none"
DEBUG_MODE=0

for arg in "$@"; do
    case $arg in
        --workers=*)
            WORKERS="${arg#*=}"
            shift
            ;;
        --skip-remediation)
            SKIP_REMEDIATION=1
            shift
            ;;
        --auto-remediate=*)
            AUTO_REMEDIATE="${arg#*=}"
            shift
            ;;
        --debug)
            DEBUG_MODE=1
            shift
            ;;
        --help)
            echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
            echo -e "${BLUE}â•‘           CourtListener Advanced Bulk Import Help             â•‘${NC}"
            echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
            echo ""
            echo -e "${BOLD}USAGE:${NC}"
            echo "  $0 [options]"
            echo ""
            echo -e "${BOLD}OPTIONS:${NC}"
            echo -e "  ${CYAN}--workers=N${NC}               Number of parallel workers (default: 1, -1 = auto-detect)"
            echo -e "  ${CYAN}--skip-remediation${NC}        Skip inconsistency remediation phase"
            echo -e "  ${CYAN}--auto-remediate=MODE${NC}     Auto-remediation mode (none, conservative, aggressive)"
            echo -e "  ${CYAN}--debug${NC}                   Enable debug logging"
            echo -e "  ${CYAN}--help${NC}                    Show this help message"
            echo ""
            echo -e "${BOLD}ENVIRONMENT VARIABLES REQUIRED:${NC}"
            echo -e "  ${YELLOW}BULK_DIR${NC}                Directory containing bulk data files"
            echo -e "  ${YELLOW}BULK_DB_HOST${NC}            Target database host"
            echo -e "  ${YELLOW}BULK_DB_USER${NC}            Target database username"
            echo -e "  ${YELLOW}BULK_DB_PASSWORD${NC}        Target database password"
            echo ""
            echo -e "${BOLD}FEATURES:${NC}"
            echo -e "  ${GREEN}â€¢${NC} Split schema loading (pre-data/post-data)"
            echo -e "  ${GREEN}â€¢${NC} Parallel table imports with resumability"
            echo -e "  ${GREEN}â€¢${NC} Inconsistency detection and reporting"
            echo -e "  ${GREEN}â€¢${NC} Automatic remediation options"
            echo -e "  ${GREEN}â€¢${NC} Comprehensive logging"
            echo -e "  ${GREEN}â€¢${NC} Automatic decompression support"
            exit 0
            ;;
    esac
done
if [[ "$WORKERS" == "-1" ]]; then
    WORKERS=$(nproc --all 2>/dev/null || echo 4)
fi

IMPORT_START_TIME=$(date +%s)

echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${BLUE}â•‘              CourtListener Advanced Bulk Import                 â•‘${NC}"
echo -e "${BLUE}â•‘                        Generated {{PIPELINE_DATE}}                        â•‘${NC}"
echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo -e "${CYAN}âš™ï¸  Configuration:${NC}"
printf "%-20s %s\n" "Workers:" "${GREEN}$WORKERS${NC}"
printf "%-20s %s\n" "Skip remediation:" "${SKIP_REMEDIATION:+${YELLOW}Yes${NC}}${SKIP_REMEDIATION:-${GREEN}No${NC}}"
printf "%-20s %s\n" "Auto remediation:" "${CYAN}$AUTO_REMEDIATE${NC}"
echo ""

# Validate environment variables
echo -e "${BLUE}â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”${NC}"
echo -e "${BLUE}â”‚                    ENVIRONMENT VALIDATION                      â”‚${NC}"
echo -e "${BLUE}â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜${NC}"

validation_errors=()

echo -n "Checking BULK_DIR... "
if [[ -z ${BULK_DIR} ]]; then
    echo -e "${RED}âŒ Not set${NC}"
    validation_errors+=("BULK_DIR is not set. BULK_DIR is where all the unzipped files are.")
elif [[ ! -d ${BULK_DIR} ]]; then
    echo -e "${RED}âŒ Directory does not exist${NC}"
    validation_errors+=("BULK_DIR directory does not exist: $BULK_DIR")
else
    echo -e "${GREEN}âœ… $BULK_DIR${NC}"
fi

echo -n "Checking BULK_DB_HOST... "
if [[ -z ${BULK_DB_HOST} ]]; then
    echo -e "${RED}âŒ Not set${NC}"
    validation_errors+=("BULK_DB_HOST is not set.")
else
    echo -e "${GREEN}âœ… $BULK_DB_HOST${NC}"
fi

echo -n "Checking BULK_DB_USER... "
if [[ -z ${BULK_DB_USER} ]]; then
    echo -e "${RED}âŒ Not set${NC}"
    validation_errors+=("BULK_DB_USER is not set.")
else
    echo -e "${GREEN}âœ… $BULK_DB_USER${NC}"
fi

echo -n "Checking BULK_DB_PASSWORD... "
if [[ -z ${BULK_DB_PASSWORD} ]]; then
    echo -e "${RED}âŒ Not set${NC}"
    validation_errors+=("BULK_DB_PASSWORD is not set.")
else
    echo -e "${GREEN}âœ… Set${NC}"
fi

if [[ ${#validation_errors[@]} -gt 0 ]]; then
    echo ""
    echo -e "${RED}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
    echo -e "${RED}â•‘                    VALIDATION FAILED                          â•‘${NC}"
    echo -e "${RED}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
    echo ""
    for error in "${validation_errors[@]}"; do
        echo -e "  ${RED}âŒ${NC} $error"
    done
    exit 1
fi

echo -e "${GREEN}ğŸ‰ All environment variables validated!${NC}"
echo ""

export BULK_DB_NAME=courtlistener
export PGPASSWORD=$BULK_DB_PASSWORD

# Set default port if not provided
if [[ -z "$PGPORT" ]]; then
    export PGPORT=5432
fi

# Initialize logging
IMPORT_LOG="$BULK_DIR/import_{{PIPELINE_DATE}}.log"
INCONSISTENCY_LOG="$BULK_DIR/inconsistency_report_{{PIPELINE_DATE}}.log"
COMPLETED_TABLES_LOG="$BULK_DIR/completed_tables.log"

echo -e "${CYAN}ğŸ“‹ Log Configuration:${NC}"
printf "%-25s %s\n" "Import log:" "${YELLOW}$IMPORT_LOG${NC}"
printf "%-25s %s\n" "Inconsistency log:" "${YELLOW}$INCONSISTENCY_LOG${NC}"
printf "%-25s %s\n" "Completed tables log:" "${YELLOW}$COMPLETED_TABLES_LOG${NC}"
echo ""

# Phase 1: Load pre-data schema (tables, types, sequences - no constraints/indexes)
echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${BLUE}â•‘                 PHASE 1: PRE-DATA SCHEMA LOADING                â•‘${NC}"
echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${CYAN}ğŸ—ï¸  Loading pre-data schema: ${YELLOW}{{PRE_DATA_SCHEMA}}${NC}"

phase1_start_time=$(date +%s)
if [[ $DEBUG_MODE -eq 1 ]]; then
    echo "DEBUG: Loading pre-data schema from: $BULK_DIR/{{PRE_DATA_SCHEMA}}"
    echo "DEBUG: Database connection: --host $BULK_DB_HOST --port $PGPORT --username $BULK_DB_USER --dbname $BULK_DB_NAME"
fi
psql -f "$BULK_DIR/{{PRE_DATA_SCHEMA}}" --host "$BULK_DB_HOST" --port "$PGPORT" --username "$BULK_DB_USER" --dbname "$BULK_DB_NAME" 2>&1 | tee -a "$IMPORT_LOG"
phase1_end_time=$(date +%s)
phase1_duration=$((phase1_end_time - phase1_start_time))

if [[ $DEBUG_MODE -eq 1 ]]; then
    echo "DEBUG: Pre-data schema loading completed in ${phase1_duration}s"
fi

echo -e "${GREEN}âœ… Pre-data schema loaded successfully in ${phase1_duration}s${NC}"
echo ""

# Define table list
TABLES=(

{{TABLE_DEFINITIONS}}
)

# Phase 2: Bulk data import (speed-optimized, resumable)
echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${BLUE}â•‘                   PHASE 2: BULK DATA IMPORT                     â•‘${NC}"
echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${CYAN}ğŸ“Š Tables to import: ${GREEN}${#TABLES[@]}${NC}"
echo -e "${CYAN}âš™ï¸  Using ${GREEN}$WORKERS${CYAN} parallel workers${NC}"
echo ""


import_table() {
    IFS='|' read -r table_name csv_filename <<< "$1"

    # Check if already completed (resumability)
    if [[ -f "$COMPLETED_TABLES_LOG" ]] && grep -q "^$table_name$" "$COMPLETED_TABLES_LOG"; then
        return 0
    fi

    # Import the table
    import_table_single "$table_name" "$csv_filename"
}

import_table_single() {
    local table_name="$1"
    local csv_filename="$2"

    if [[ -z "$csv_filename" ]]; then
        echo "ERROR: No CSV filename provided for $table_name. Skipping import." | tee -a "$IMPORT_LOG"
        return 1
    fi

    # Use wildcard pattern to match any file (compressed or uncompressed)
    # Try exact match first, then fall back to pattern matching
    local compressed_file=$(ls "$BULK_DIR/$csv_filename.bz2" 2>/dev/null | head -n 1)
    local uncompressed_file=$(ls "$BULK_DIR/$csv_filename" 2>/dev/null | head -n 1)

    # If exact match fails, try pattern matching (handles pluralization issues)
    if [[ -z "$compressed_file" && -z "$uncompressed_file" ]]; then
        # Extract the base name without date and extension for pattern matching
        local base_pattern=$(echo "$csv_filename" | sed 's/-[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}\.csv$//')
        compressed_file=$(ls "$BULK_DIR/${base_pattern}"*".csv.bz2" 2>/dev/null | head -n 1)
    local compressed_file
    local uncompressed_file
    compressed_file=("$BULK_DIR/$csv_filename.bz2")
    uncompressed_file=("$BULK_DIR/$csv_filename")
    compressed_file="${compressed_file[0]}"
    uncompressed_file="${uncompressed_file[0]}"
    
    # If exact match fails, try pattern matching (handles pluralization issues)
    if [[ -z "$compressed_file" && -z "$uncompressed_file" ]]; then
        # Extract the base name without date and extension for pattern matching
        local base_pattern=$(echo "$csv_filename" | sed 's/-[0-9]\{4\}-[0-9]\{2\}-[0-9]\{2\}\.csv$//')
        compressed_file=("$BULK_DIR/${base_pattern}"*.csv.bz2)
        uncompressed_file=("$BULK_DIR/${base_pattern}"*.csv)
        compressed_file="${compressed_file[0]}"
        uncompressed_file="${uncompressed_file[0]}"
    fi

    local import_file=""
    local cleanup_file=""

    if [[ -n "$compressed_file" && -f "$compressed_file" ]]; then
        # File is compressed, decompress it on-demand
        import_file="${compressed_file%.bz2}"
        cleanup_file="$import_file"

        if [[ $DEBUG_MODE -eq 1 ]]; then
            echo "DEBUG: Decompressing $compressed_file to $import_file using lbzip2 with 4 workers"
        fi
        echo "INFO: Decompressing $compressed_file for $table_name import..." | tee -a "$IMPORT_LOG"

        # Use lbzip2 with 4 workers for efficient decompression
        if ! lbzip2 -d -k -n 4 "$compressed_file"; then
            echo "ERROR: Failed to decompress $compressed_file" | tee -a "$IMPORT_LOG"
            return 1
        fi

    elif [[ -n "$uncompressed_file" && -f "$uncompressed_file" ]]; then
        # File is already uncompressed
        import_file="$uncompressed_file"

    else
        echo "ERROR: No matching file for $csv_filename (tried both .csv and .csv.bz2) in $BULK_DIR" | tee -a "$IMPORT_LOG"
        return 1
    fi

    if [[ $DEBUG_MODE -eq 1 ]]; then
        echo "DEBUG: Starting import of $table_name from $import_file"
        echo "DEBUG: Database connection: --host $BULK_DB_HOST --port $PGPORT --username $BULK_DB_USER --dbname $BULK_DB_NAME"
    fi
    echo "INFO: Import command for $table_name: \\COPY public.$table_name FROM '$import_file'" | tee -a "$IMPORT_LOG"

    # Import the table
    local import_result=0
    psql --host "$BULK_DB_HOST" --port "$PGPORT" --username "$BULK_DB_USER" --dbname "$BULK_DB_NAME" 2>&1 <<EOF | tee -a "$IMPORT_LOG"
BEGIN;
SET session_replication_role = 'replica';
\\COPY public.$table_name FROM '$import_file' WITH (FORMAT csv, ENCODING utf8, ESCAPE '\\', HEADER);
SET session_replication_role = 'origin';
COMMIT;
EOF
    import_result=${PIPESTATUS[0]}

    # Clean up decompressed file if we created it
    if [[ -n "$cleanup_file" && -f "$cleanup_file" ]]; then
        if [[ $DEBUG_MODE -eq 1 ]]; then
            echo "DEBUG: Cleaning up decompressed file $cleanup_file"
        fi
        echo "INFO: Cleaning up decompressed file $cleanup_file" | tee -a "$IMPORT_LOG"
        rm -f "$cleanup_file"
    fi

    if [[ $import_result -eq 0 ]]; then
        # Mark as completed
        echo "INFO: Table $table_name import completed." | tee -a "$IMPORT_LOG"
        echo "$table_name" >> "$COMPLETED_TABLES_LOG"
    else
        echo "ERROR: Table $table_name import failed." | tee -a "$IMPORT_LOG"
        return 1
    fi
}

export -f import_table
export -f import_table_single
export BULK_DIR BULK_DB_HOST BULK_DB_USER BULK_DB_NAME PGPASSWORD IMPORT_LOG COMPLETED_TABLES_LOG

echo "Starting speed-optimized parallel import with $WORKERS worker(s)..."
if command -v parallel >/dev/null 2>&1; then
    PARALLEL_JOBS=$WORKERS
    if [[ "$WORKERS" == "-1" ]]; then
        PARALLEL_JOBS=$(nproc --all 2>/dev/null || echo 4)
    fi
    echo "Using GNU Parallel for table imports with $PARALLEL_JOBS jobs. Progress reporting enabled."
    printf "%s\n" "${TABLES[@]}" | parallel --jobs "$PARALLEL_JOBS" --bar --joblog "$BULK_DIR/table_import_joblog.txt" --env BULK_DIR,BULK_DB_HOST,BULK_DB_USER,BULK_DB_NAME,PGPASSWORD,IMPORT_LOG,COMPLETED_TABLES_LOG import_table {}
else
    echo "WARNING: GNU Parallel not found. Falling back to xargs -P. Progress reporting will NOT be available."
    printf "%s\n" "${TABLES[@]}" | xargs -P "$WORKERS" -I {} bash -c 'import_table "{}"'
fi
echo "Bulk data import complete."
echo ""

# Phase 3: Inconsistency detection (PLAN.md requirement)
echo "PHASE 3: Inconsistency detection..."
echo "Running data integrity checks..." | tee -a "$INCONSISTENCY_LOG"

# Basic table row counts
echo "=== TABLE ROW COUNTS ===" >> "$INCONSISTENCY_LOG"
psql --host "$BULK_DB_HOST" --username "$BULK_DB_USER" --dbname "$BULK_DB_NAME" --command "
SELECT schemaname, tablename, n_tup_ins as estimated_rows
FROM pg_stat_user_tables
WHERE schemaname = 'public'
ORDER BY tablename;
" >> "$INCONSISTENCY_LOG" 2>&1

# Foreign key constraint violations (examples - add more as needed)
echo "" >> "$INCONSISTENCY_LOG"
echo "=== FOREIGN KEY VIOLATIONS ===" >> "$INCONSISTENCY_LOG"

# Check for orphaned records in key relationships
echo "Checking orphaned positions..." >> "$INCONSISTENCY_LOG"
psql --host "$BULK_DB_HOST" --username "$BULK_DB_USER" --dbname "$BULK_DB_NAME" --command "
SELECT 'people_db_position.person_id orphans' as check_type, count(*) as violations
FROM people_db_position p
WHERE p.person_id IS NOT NULL
  AND p.person_id NOT IN (SELECT id FROM people_db_person);
" >> "$INCONSISTENCY_LOG" 2>&1

echo "Checking orphaned dockets..." >> "$INCONSISTENCY_LOG"
psql --host "$BULK_DB_HOST" --username "$BULK_DB_USER" --dbname "$BULK_DB_NAME" --command "
SELECT 'search_docket.originating_court_information_id orphans' as check_type, count(*) as violations
FROM search_docket d
WHERE d.originating_court_information_id IS NOT NULL
  AND d.originating_court_information_id NOT IN (SELECT id FROM search_originatingcourtinformation);
" >> "$INCONSISTENCY_LOG" 2>&1

echo "Checking orphaned opinions..." >> "$INCONSISTENCY_LOG"
psql --host "$BULK_DB_HOST" --username "$BULK_DB_USER" --dbname "$BULK_DB_NAME" --command "
SELECT 'search_opinion.cluster_id orphans' as check_type, count(*) as violations
FROM search_opinion o
WHERE o.cluster_id IS NOT NULL
  AND o.cluster_id NOT IN (SELECT id FROM search_opinioncluster);
" >> "$INCONSISTENCY_LOG" 2>&1

echo "Checking orphaned citations..." >> "$INCONSISTENCY_LOG"
psql --host "$BULK_DB_HOST" --username "$BULK_DB_USER" --dbname "$BULK_DB_NAME" --command "
SELECT 'search_citation.cluster_id orphans' as check_type, count(*) as violations
FROM search_citation c
WHERE c.cluster_id IS NOT NULL
  AND c.cluster_id NOT IN (SELECT id FROM search_opinioncluster);
" >> "$INCONSISTENCY_LOG" 2>&1

# Duplicate primary key checks
echo "" >> "$INCONSISTENCY_LOG"
echo "=== DUPLICATE PRIMARY KEY CHECKS ===" >> "$INCONSISTENCY_LOG"
for table in people_db_person search_court search_docket search_opinioncluster search_opinion; do
    echo "Checking duplicates in $table..." >> "$INCONSISTENCY_LOG"
    psql --host "$BULK_DB_HOST" --username "$BULK_DB_USER" --dbname "$BULK_DB_NAME" --command "
    SELECT '$table duplicates' as check_type, count(*) - count(DISTINCT id) as duplicate_ids
    FROM $table;
    " >> "$INCONSISTENCY_LOG" 2>&1
done

echo "Inconsistency detection complete. Results in: $INCONSISTENCY_LOG"

# Phase 4: Remediation (PLAN.md requirement)
if [[ "$SKIP_REMEDIATION" != "1" ]]; then
    echo ""
    echo "PHASE 4: Data remediation..."

    # Show inconsistency summary
    echo "Inconsistency Summary:"
    echo "====================="
    grep -E "(violations|duplicate_ids)" "$INCONSISTENCY_LOG" | grep -v " 0$" || echo "No major inconsistencies detected."

    # Automatic remediation options
    case "$AUTO_REMEDIATE" in
        delete_orphans)
            echo "AUTO-REMEDIATION: Deleting orphaned records..."
            # Add specific deletion queries here
            echo "Orphan deletion would be implemented here."
            ;;
        create_placeholders)
            echo "AUTO-REMEDIATION: Creating placeholder records..."
            # Add placeholder creation queries here
            echo "Placeholder creation would be implemented here."
            ;;
        *)
            echo "MANUAL REMEDIATION REQUIRED:"
            echo "Available options:"
            echo "  --auto-remediate=delete_orphans     : Delete orphaned records"
            echo "  --auto-remediate=create_placeholders: Create placeholder parent records"
            echo "  --skip-remediation                  : Skip this phase entirely"
            echo ""
            echo "Review the inconsistency log and re-run with appropriate remediation option."
            echo "Or continue to constraint application if inconsistencies are acceptable."
            echo ""
            echo "Import stopped at remediation phase. This script will not continue interactively."
            echo "Re-run this script with remediation options when ready."
            exit 0
            ;;
    esac
else
    echo "PHASE 4: Remediation skipped (--skip-remediation flag used)"
fi

# Phase 5: Apply post-data schema (constraints, indexes, triggers)
echo ""
echo "PHASE 5: Applying post-data schema (constraints, indexes, triggers)..."
echo "Loading post-data schema: {{POST_DATA_SCHEMA}}"
if [[ $DEBUG_MODE -eq 1 ]]; then
    echo "DEBUG: Loading post-data schema from: $BULK_DIR/{{POST_DATA_SCHEMA}}"
    echo "DEBUG: Database connection: --host $BULK_DB_HOST --port $PGPORT --username $BULK_DB_USER --dbname $BULK_DB_NAME"
fi
psql -f "$BULK_DIR/{{POST_DATA_SCHEMA}}" --host "$BULK_DB_HOST" --port "$PGPORT" --username "$BULK_DB_USER" --dbname "$BULK_DB_NAME" 2>&1 | tee -a "$IMPORT_LOG"
if [[ $DEBUG_MODE -eq 1 ]]; then
    echo "DEBUG: Post-data schema applied successfully"
fi
echo "Post-data schema applied successfully."
echo ""

# Final validation
echo "FINAL VALIDATION: Running constraint checks..."
psql --host "$BULK_DB_HOST" --username "$BULK_DB_USER" --dbname "$BULK_DB_NAME" --command "
SELECT 'constraint_violations' as check_type, count(*) as violations
FROM information_schema.table_constraints tc
WHERE tc.constraint_type = 'FOREIGN KEY'
  AND tc.table_schema = 'public';
" | tee -a "$IMPORT_LOG"

echo ""
echo "=========================================="
echo "BULK IMPORT COMPLETE!"
echo "=========================================="
echo "Import log: $IMPORT_LOG"
echo "Inconsistency log: $INCONSISTENCY_LOG"
echo "Completed tables: $COMPLETED_TABLES_LOG"
echo ""
echo "The database is now ready for use."
