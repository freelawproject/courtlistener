\label{techdecisions}
\section{Technical decisions}
In designing the CourtListener.com platform, I made many technical decisions. In this section, I will delve deeply into a few of the more difficult decisions, and will provide an overview of the reasoning behind their final outcomes.\footnote{As was mentioned in the introduction, since this is an open source project, all of the code is available online under the third version of the GNU Affero General Public License. This license was chosen because it allows people to copy and use the code for free, but requires that they publicly share any modifications that they make to it. The more-common GNU General Public License (GPL) similarly allows the code to be used at no cost, but only requires that changes to the code be shared with the public if the resulting program itself is distributed to the public. Because this project is server-based, the program is never technically distributed, and so might not have the same protections if covered by the GNU GPL. The Affero General Public License closes this loophole by requiring all modifications to the code be shared. To browse the code, please see: http://bitbucket.org/mlissner/legal-current-awareness.}

Going into the creation process, some initial decisions were made simply to limit the possibilities. Because of my previous experience with Python and the Linux, Apache, MySQL stack, I decided early on to use these technologies. Building on this, two major decisions had to be made. First, I had to determine the best search engine, and second, I had to decide on a web framework to use as my Object Relational Model (ORM) and templating engine. 

For the question of which search engine to use, I completed an in-depth review of every open source search engine I could find.\footnote{For details please see the spreadsheet located in the project repository, at http://bitbucket.org/mlissner/legal-current-awareness/raw/b35105d6a233/Documents /Search\%20Engine\%20Analysis,\%202010-02-06.ods} I examined each search engine along 16 dimensions, including community support, documentation, features, license, and code base size, among others. Once I had identified the three open source search engines that appeared the best, I took a close look at their Boolean support, simplicity of design, and features. Ultimately, I decided on Sphinx Search because it has sophisticated Boolean support,\footnote{For details of the Boolean syntax supported, see http://courtlistener.com/search/ advanced-techniques.} a relatively small code base size, an active community, and an engaged developer.\footnote{http://sphinxsearch.com/} This decision has worked out well, as it was possible to link Sphinx directly to the MySQL database, and it provides very fast and accurate search results, even for very complicated queries. An unanticipated side-effect of using such a powerful search engine is that it builds a very large search index. Numerous times during the corpus aggregation phase, the index filled the entire hard drive, and a larger plan with the server provider had to be purchased. This problem has largely been solved by removing some of Sphinx's more powerful search capabilities, such as infix searching, and by implementing a main+delta reindexing scheme. 

The main+delta reindexing scheme creates two indexes that Sphinx searches. The first is the main index, which contains full-text search indexes for about 130,000 legal opinions, and is currently about 4.1GB in size. Recreating this index currently takes the server about an hour to complete, during which time a copy of the index is created, thus doubling it in size. The second index -- the so-called delta index -- contains only the newest documents, is about 20MB in size, and takes about a minute to reindex. Thus, each hour, it is possible for the indexer to add new documents to the delta index, and once every two months, in the middle of the night, the two indexes are merged.


The second decision that greatly shaped the development of the project was to use Django as the web framework.\footnote{http://www.djangoproject.com/} This decision was made in part because I had used it in the past, and in part because it supported all of the features that were on the MoSCoW analysis mentioned in section 2. This decision has worked out well, as many of the more complicated features of the site, such as pagination of search results, syndication, form creation and validation, and security are all built into Django. Not having to worry about data validation or more complicated things such as cross-site request forgeries (CSRF) made building the features of the site more appealing and streamlined.\footnote{The Open Web Application Security Project identifies CSRF as a ``Widely prevalent'' security weakness, and lists it as number five on its top ten list of Critical Web Application Security Risks. \cite{open_web_application_security_project_owasp_2010}} An additional benefit of the Django framework is the admin interface that it provides: on the back end, it is possible to browse and edit all of the data in the system, and creating tie-ins on the front end for content administration is under way, with each document in the corpus having an ``Edit'' link available to administrators in the side navigation panel.

One of the more complicated features of CourtListener.com is its pluggable court scraper and PDF extractor. This part of the platform has undergone many iterations, starting with a basic scraper that crashed regularly and silently, and ending at its current version as a multi-threaded daemon that is running all the time on the server, and which downloads the latest opinions -- on average -- within 15 minutes of their posting. Designing the scraper to be reliable, efficient and have low bandwidth requirements has been a major challenge. The current implementation can be started in PDF parse and/or scrape mode, has three verbosity levels (debug, chatty and silent), can be told which courts to scrape, and uses the following algorithm:
\begin{enumerate}
    \item{Download the HTML of the court website, and generate a digital fingerprint of it. Check that fingerprint in the database to see if the site has changed.}
    \item{If the site has changed, build a tree out of its HTML, and use XPath to identify the relevant leaves of the tree to analyze. If it has not, move to the next court.}
    \item{Begin downloading the first PDF opinion from the site, and generate a digital fingerprint of it. If the fingerprint is already in the database, move to the next PDF. If three PDFs in a row are already in the database, move to the next court.}
    \item{If the fingerprint of the PDF is not already in the database, parse the leaves of the tree, and extract and format the relevant information from them. Once all the information has been successfully extracted place it all in the database, and save the PDF to disk.}
    \item{Extract the text from any downloaded PDFs, sleep for a few minutes, then repeat this process for each PDF in each court requested.}
\end{enumerate}
The result of this algorithm is that each court is visited about once every half hour, and changes to the court website are identified at that time. Since PDFs are large files, this minimizes the number of PDFs that are downloaded, and duplicates are eliminated at the source.

Another major issue that I have encountered has been scaling the site. Since the site now contains the almost the entire Supreme Court record, and thousands of documents for other courts, completing tasks such as a simple lookup of a record in the database have begun to slow down. The solution to this has been to aggressively implement database caching and indexing and front end caching through memcached for users that are not logged in. This has eliminated much of the latency problem that the platform initially had, but some queries need to be optimized manually. MySQL is currently logging any query that takes too long to finish, and I will be analyzing the results of this log soon.
